{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar Lander con Deep Q Learning\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "- Juan Jose Urioste\n",
    "- Carlos Huerta Garcia\n",
    "- Alejo Torres Teruel\n",
    "\n",
    "## Instalacion\n",
    "\n",
    "Todas las dependencias necesarias estan listadas en el fichero `requirements.txt`. Que pueden ser instaladas con un manejador de paquetes de python como `pip` o `conda`.\n",
    "\n",
    "## DevContainer\n",
    "\n",
    "Ademas se provee una descripcion del Entorno de desarrollo utilizado para trabajar con el entorno de gymnasium.\n",
    "\n",
    "como comando de post creacion se especifica la instalacion y compilacion de las dependencias necesarias para el entorno de gymnasium y el proyecto.\n",
    "\n",
    "\n",
    "## Importamos las librerias necesarias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 08:34:01.826484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from collections import deque\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agente de DQL\n",
    "\n",
    "la clase DQLAgent se encargara de manejar la logica de entrenamiento y evaluacion del agente.\n",
    "Va a depender del entorno sobre el cual se ejecute y agrupa los metodos y los atributos necesarios para implementar el algoritmo de DQL.Mas en especifico, en este caso se trata de DDQL ya que contamos con un modelo de target y otro de entrenamiento.\n",
    "\n",
    "\n",
    "## Atributos\n",
    "\n",
    "- `state_size`: dimension del espacio de estados\n",
    "- `action_size`: dimension del espacio de acciones\n",
    "\n",
    "Ambos son dependientes de entorno sobre el cual se ejecute el agente.\n",
    "\n",
    "- `epsilon`: factor de exploracion\n",
    "- `epsilon_decay`: factor de decaimiento de epsilon -> este factor se utiliza para actualziar el valor de epsilon en cada episodio esto con el fin de que el agente explore menos y explote mas a medida que avanza el entrenamiento\n",
    "- `epsilon_min`: valor minimo de epsilon\n",
    "- `learning_rate`: factor de aprendizaje -> a fin de cuenta nuestros modelos son modelos de Deep learning en este caso perceptrones multicapas con capas full conected por lo que el aprendizaje se realiza mediante el algoritmo de SGD y este factor es el que determina la magnitud de los cambios en los pesos de las conexiones de las neuronas\n",
    "- `memory`: Es una estructura de datos optimizada para accesar los datos de sus extremos, en este caso se utiliza para almacenar las transiciones de la forma (estado, accion, recompensa, estado siguiente, done) y poder muestrear de forma aleatoria hacer que el modelo \"reviva\" experiencias pasadas y pueda aprender de ellas\n",
    "- `model`: modelo de entrenamiento sobre el cual se va a realizar el aprendizaje\n",
    "- `target_model`: modelo de target que se actualiza cada cierta cantidad de episodios y es aquel que se utiliza para predecir el valor de Q en el estado siguiente\n",
    "\n",
    "\n",
    "## Metodos\n",
    "\n",
    "- `build_model`: construye el modelo de Deep learning que se va a utilizar para entrenar el agente\n",
    "- `remember`: almacena una transicion en la memoria del agente\n",
    "- `act`: retorna la accion a tomar en el estado actual infiriendola en base al modelo\n",
    "- `replay`: muestrea la memoria de manera aleatoria y crea un batch usando aquellas memorias que no terminaron el episodio, predecimos usando ambos modelos y sobre las memorias donde no termino el episodio actualizamos los valores Q de acuerdo a la formula de DQL, luego recuperamos las acciones desde el batch al igual que los targets que se actualizaran con el valor calculado anteriormente para luego entrenar el modelo. vamos a entrenar en un epoch ya que este procedimiento sucede en cada en cada step de cada episodio\n",
    "- `adaptiveEGreedy`: Nos permite ajustar el factor de exploracion\n",
    "- `targetModelUpdate`: Actualizamos los pesos del modelo de target con los pesos del modelo de entrenamiento. Ambos cuentan con la misma arquitectura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self, env):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9993\n",
    "        self.gamma = 0.99\n",
    "        self.learning_rate = 0.0001\n",
    "        self.memory = deque(maxlen=4000)\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, s):\n",
    "        if np.random.rand() <= self.epsilon: #! epsilon greedy\n",
    "            return np.random.choice(self.action_size)\n",
    "        act_values = self.model.predict(s)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self,batch_size):\n",
    "        if len(agent.memory) < batch_size: #! no tengo suficientes memorias\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size) #! samplear la memoria tomando como tamaño batch_size\n",
    "        minibatch = np.array(minibatch)\n",
    "        not_done_indices = np.where(minibatch[:, 4] == False) #! me interesa saber los que no termino \n",
    "        y = np.copy(minibatch[:, 2])\n",
    "\n",
    "        if len(not_done_indices[0]) > 0:\n",
    "            predict_sprime = self.model.predict(np.vstack(minibatch[:, 3]))\n",
    "            predict_sprime_target = self.target_model.predict(np.vstack(minibatch[:, 3]))\n",
    "            \n",
    "            y[not_done_indices] += np.multiply(self.gamma, predict_sprime_target[not_done_indices, np.argmax(predict_sprime[not_done_indices, :][0], axis=1)][0])\n",
    "\n",
    "        actions = np.array(minibatch[:, 1], dtype=int)\n",
    "        y_target = self.model.predict(np.vstack(minibatch[:, 0]))\n",
    "        y_target[range(batch_size), actions] = y\n",
    "        self.model.fit(np.vstack(minibatch[:, 0]), y_target, epochs=1, verbose=0)\n",
    "        \n",
    "\n",
    "            \n",
    "    def adaptiveEGreedy(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def targetModelUpdate(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento\n",
    "\n",
    "- Creamos el entorno e instanciamos el agente\n",
    "- definimos el tamaño del batch y la cantidad de episodios a entrenar\n",
    "- Por cada episodio vamos a resetear el entorno y el estado del agente\n",
    "- En cada step vamos a pedir una accion inferida por el a gente para ejecutarse en el entorno, luego memorizar la transicion y realizar el proceso de replay\n",
    "\n",
    "- si el episodio finaliza vamos a actualizar el modelo de target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "state, _ = env.reset()\n",
    "\n",
    "agent = DQLAgent(env)\n",
    "state_number = env.observation_space.shape[0]\n",
    "\n",
    "batch_size = 32\n",
    "episodes = 5\n",
    "for e in range(episodes):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_number])\n",
    "    total_reward = 0\n",
    "    for time in range(1000):\n",
    "\n",
    "        action = agent.act(state)\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_number])\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            agent.targetModelUpdate()\n",
    "            break\n",
    "\n",
    "    agent.adaptiveEGreedy()\n",
    "\n",
    "    print('Episode: {}, Reward: {}'.format(e, total_reward))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probamos el modelo\n",
    "ejecutamos un bucle PPA pero con un entorno en modo de renderizado humano para poder visualizar el comportamiento del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "\n",
    "state, _ = env.reset()\n",
    "\n",
    "terminated = truncated = False\n",
    "while not (terminated or truncated):\n",
    "    action = agent.act(state)\n",
    "    state, _, terminated, truncated, _ = env.step(action)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
