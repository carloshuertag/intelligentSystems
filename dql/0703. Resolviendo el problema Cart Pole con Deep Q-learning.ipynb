{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TT3LPGc5PuK"
   },
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Resolviendo el problema Cart Pole con Deep $Q$-learning<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto D铆az lvarez<br>ltima actualizaci贸n: 2023-04-19</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXBzOdaLAEUn"
   },
   "source": [
    "## Introducci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementaci贸n m谩s sencilla del $Q$-Learning utiliza una tabla para guardar todos los valores, pero esto se convierte r谩pidamente en inviable para entornos con muchos estados o acciones posibles. Un entorno con un espacio de observaciones continuo (pr谩cticamente cualquiera) es uno de esos ejemplos, ya que discrretizar el espacio de acciones dispara el espacio requerido para la tabla en memoria.\n",
    "\n",
    "Utilizando un modelo de red neuronal (profunda) para representar la funci贸n $Q(s, a)$, el agente puede generalizar mejor combinaciones no experimentadas de $(s, a)$. La entrada a este modelo ser铆a una observaci贸n, mientras que la salida ser铆a un valor $Q$ para cada acci贸n permitida en ese estado. Esta implementaci贸n se denomina Deep $Q$-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-YSpziT0K9I"
   },
   "source": [
    "Vamos a tratar de resolver el problema del p茅ndulo invertido que se implementan en el entorno [Cart Pole de Gymnasium](https://www.gymlibrary.dev/environments/classic_control/cart_pole/). Aplicaremos la t茅cnica de _Deep $Q$-learning_, la cual es la indicada en el caso de procesos en los que el espacio de estados en el que trabajar es continuo.\n",
    "\n",
    "![](https://blazaid.github.io/Aprendizaje-profundo/Notebooks//cart-pole.gif)\n",
    "\n",
    "El espacio de estados se representa con un vector de 4 dimensiones donde las posiciones representan lo siguiente:\n",
    "0. Posici贸n del carrito (de -4.8 a 4.8)\n",
    "1. Velocidad del carrito (de $-\\infty$ a $\\infty$)\n",
    "2. ngulo del p茅ndulo (de -0.418 a 0.418)\n",
    "3. Velocidad angular del p茅ndulo (de $-\\infty$ a $\\infty$)\n",
    "\n",
    "El espacio de acciones es un vector de dos dimensiones donde cada posici贸n representa:\n",
    "\n",
    "0. Empujar al carrito a la izquierda\n",
    "1. Empujar el carrito a la derecha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports y configuraci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci贸n importaremos las bibliotecas que se usar谩n a lo largo del _notebook_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjLS1WetFhCE"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import os\n",
    "import random\n",
    "from typing import Any, Callable, List, NamedTuple, Sequence, SupportsFloat, Union\n",
    "\n",
    "# Desactivamos los warnings de tensorflow, que son un poco cargantes\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asimismo, configuramos algunos par谩metros para adecuar la presentaci贸n gr谩fica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (15, 8),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementos auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la implementaci贸n ser谩 un poco m谩s larga que el _notebook_ anterior, haremos uso de una serie de elementos auxiliares que nos ayudar谩n ha hacer el c贸digo m谩s manejable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transiciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos definiendo la clase `Transition`, que representar谩 la transici贸n que ocurre de un estado $s_t$ a un estado $s_{t+1}$.\n",
    "\n",
    "Dicha transici贸n contendr谩 la informaci贸n de qu茅 acci贸n provoc贸 la transici贸n, cu谩l fue la recompensa de haberla llevado a cabo y si se ha terminado, ya sea por llegar a un estado terminal (p.ej. llegar al destino o morir) o por cualquier otra raz贸n (p.ej llegar a un determinado l铆mite de tiempo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(NamedTuple):\n",
    "    \"\"\"Representa la transici贸n de un estado al siguiente\"\"\"\n",
    "    prev_state: Any       # Estado origen de la transici贸n\n",
    "    next_state: Any       # Estado destino de la transici贸n\n",
    "    action: Any           # Acci贸n que provoc贸 esta transici贸n\n",
    "    reward: SupportsFloat # Recompensa obtenida\n",
    "    terminated: bool      # Si se ha llegado a un estado terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las propiedades con las que cuentan los agentes al ser entrenados con $Q$-Learning es la memoria de experiencias, representadas como transiciones entre estados. sta se usar谩 durante la fase de entrenamiento.\n",
    "\n",
    "La implementaremos como un buffer de tama帽o opcionalmente limitado. Generalmente se acota el tama帽o de la memoria, reemplazando las memorias m谩s antig眉as con las m谩s recientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"Representa la memoria de un agente.\n",
    "\n",
    "    Concretamente, almacenar谩 las 煤ltimas n transiciones realizadas en\n",
    "    el entorno. El tama帽o de la memoria se establecer谩 en el momento de\n",
    "    crear la memoria del mismo.\n",
    "\n",
    "    La memoria guarda las transiciones de manera ordenada, y se podr谩\n",
    "    acceder a ellos por 铆ndice, de manera que el recuerdo m谩s lejano\n",
    "    estar谩 en la posici贸n 0 y el m谩s reciente en la posici贸n -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int):\n",
    "        \"\"\"Inicializa el objeto.\n",
    "\n",
    "        :param size: El tama帽o m谩ximo de la memoria del agente.\"\"\"\n",
    "        self.max_size = int(size)\n",
    "        self.transitions: deque = deque(maxlen=self.max_size)\n",
    "\n",
    "    def remember(self, transition: Transition):\n",
    "        \"\"\"A帽ade un nuevo recuerdo a la memoria del agente.\n",
    "\n",
    "        :param transition: La transici贸n a recordar.\"\"\"\n",
    "        self.transitions.append(transition)\n",
    "\n",
    "    def batch(self, n: int) -> List[Transition]:\n",
    "        \"\"\"Devuelve n recuerdos aleatorios de la memoria.\n",
    "\n",
    "        :param n: El n煤mero de recuerdos aleatorios a devolver. Si es\n",
    "            superior al n煤mero de recuerdos totales devolver谩 todos los\n",
    "            recuerdos almacenados.\n",
    "        :returns: La lista de transiciones.\n",
    "        \"\"\"\n",
    "        n = min(len(self.transitions), n)\n",
    "        return random.sample(self.transitions, n)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"El n煤mero de recuerdos que contiene esta memoria.\n",
    "\n",
    "        :returns: Un entero mayor o igual a 0.\n",
    "        \"\"\"\n",
    "        return len(self.transitions)\n",
    "\n",
    "    def __getitem__(\n",
    "            self,\n",
    "            key: Union[int, slice]\n",
    "    ) -> Union[Transition, Sequence[Transition]]:\n",
    "        \"\"\"Devuelve el/los elemento/s especificados.\n",
    "\n",
    "        :param key: El argumento que indica los elementos. Puede ser un\n",
    "            entero normal o un slice.\n",
    "        :returns: El/los elemento/s especificados por el 铆ndice.\n",
    "        \"\"\"\n",
    "        return self.transitions.__getitem__(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaci贸n del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro agente se parecer谩 a los agentes que hemos implementado. Primero, porque comparten el hecho de que se tratan de agentes interactuando con un entorno (percibir $\\rightarrow$ actuar), y segundo, porque _Deep $Q$-learning_ es una extensi贸n de _$Q$-learning_, por lo que algo se tiene que parecer.\n",
    "\n",
    "Vamos con la implementaci贸n y posteriormente explicaremos los detalles m谩s importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZ6P4Gj0FtnU"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(\n",
    "        self, *,\n",
    "        env: gymnasium.Env,\n",
    "        model: Union[Callable[[int, int], tf.keras.Model], tf.keras.Model, str],\n",
    "        batch_size=32,\n",
    "        memory_size: int = 1e5,\n",
    "        gamma=0.99,\n",
    "    ):\n",
    "        \"\"\"Inicializa el objeto.\n",
    "        :param model: El modelo del objeto. Puede ser una funci贸n que devuelva\n",
    "            un nuevo modelo (compilado), un objeto de `tf.keras.Model` ya\n",
    "            existente (en cuyo caso se clonar谩) o una cadena con un path\n",
    "            v谩lido, en cuyo caso se cargar谩 de disco.\n",
    "        \"\"\"\n",
    "        # El entorno en el que vamos a trabajar\n",
    "        self.env = env\n",
    "        self.num_inputs = env.observation_space.shape[0]\n",
    "        self.num_outputs = env.action_space.n\n",
    "\n",
    "        # El modelo de aprendizaje de nuestro agente\n",
    "        if callable(model):\n",
    "            self.model = model(\n",
    "                env.observation_space.shape[0],\n",
    "                env.action_space.n,\n",
    "            )\n",
    "        elif isinstance(model, tf.keras.models.Model):\n",
    "            self.model = model\n",
    "        elif isinstance(model, str):\n",
    "            self.model = tf.keras.models.load_model(model)\n",
    "        else:\n",
    "            raise ValueError('Valid models are a function, a model or a path')\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.memory = Memory(size=1e5)\n",
    "\n",
    "        # El estado del entorno en el que se encuentra el agente\n",
    "        self.current_state = None\n",
    "        # El n煤mero de paso en la tarea en la que se encuentra el agente\n",
    "        self.current_step = 0\n",
    "\n",
    "    def episode(self, epsilon=0, max_iterations=None):\n",
    "        max_iterations = max_iterations or np.inf\n",
    "\n",
    "        self.current_state, _ = self.env.reset()\n",
    "        self.current_step = 0\n",
    "\n",
    "        reward = 0\n",
    "        running = True\n",
    "        while running and self.current_step < max_iterations:\n",
    "            self.current_step += 1\n",
    "\n",
    "            perception = self.perceive()\n",
    "            action = self.decide(perception, epsilon)\n",
    "            transition = self.act(action)\n",
    "            self.learn()\n",
    "\n",
    "            reward += transition.reward\n",
    "            running = not transition.terminated\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def perceive(self):\n",
    "        return self.current_state\n",
    "        \n",
    "    def decide(self, perception, epsilon=0):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return random.randrange(environment.action_space.n)\n",
    "        else:\n",
    "            perception = perception[np.newaxis, ...]\n",
    "            q_values = self.model.predict(perception, verbose=0)\n",
    "            return np.argmax(q_values[0])\n",
    "    \n",
    "    def act(self, action):\n",
    "        # Ejecutamos la acci贸n sobre el entorno\n",
    "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Guardamos en la memoria del agente la transici贸n realizada junto con\n",
    "        # su informaci贸n\n",
    "        self.memory.remember(Transition(\n",
    "            prev_state=self.current_state,\n",
    "            next_state=next_state,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            terminated=terminated,\n",
    "        ))\n",
    "        \n",
    "        # Indicamos el nuevo estado en el que se encuentra el agente ahora\n",
    "        self.current_state = next_state\n",
    "        \n",
    "        return self.memory[-1]\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            transactions = self.memory.batch(self.batch_size)\n",
    "            \n",
    "            prev_states = np.array([t.prev_state for t in transactions])\n",
    "            next_states = np.array([t.next_state for t in transactions])\n",
    "\n",
    "            qs = self.model.predict(prev_states, verbose=0)\n",
    "            next_qs = self.model.predict(next_states, verbose=0)\n",
    "\n",
    "            for i, (txn, next_q) in enumerate(zip(transactions, next_qs)):\n",
    "                if (txn.terminated):\n",
    "                    qs[i][txn.action] = txn.reward\n",
    "                else:\n",
    "                    qs[i][txn.action] = txn.reward + self.gamma * np.max(next_q)    \n",
    "\n",
    "            self.model.train_on_batch(prev_states, qs)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a recorrer ciertos detalles de la implementaci贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En entrenamiento supervisado y no supervisado, los _epochs_ son una vuelta entera sobre todo el conjunto de datos para el entrenamiento del modelo. En aprendizaje por refuerzo su hom贸logo son los **episodios** o tareas, los cuales son una ejecuci贸n del agente sobre el entorno hasta que 茅ste finaliza su tarea (buen o mal). Esta ejecuci贸n se realiza en el m茅todo `episode`\n",
    "\n",
    "El agente, para aprender, ejecutar谩 tareas una detr谩s de otra. Cada una de ellas constar谩 de un ciclo constante de percibir (m茅todo `perceive`) el entorno, decidir la acci贸n a ejecutar (m茅todo `decide`), ejecutar dicha acci贸n sobre el entorno (m茅todo `act`), y aprender sobre lo vivido (m茅todo `learn`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percibir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro problema vamos a usar como percepci贸n exclusivamente el estado devuelto por el entorno, aunque en otros casos podr铆a usarse otra informaci贸n adicional (por ejemplo el n煤mero de iteraciones que llevamos dentro del episodio para que aprenda a tener prisa si se est谩 acabando el tiempo, por ejemplo).\n",
    "\n",
    "\n",
    "```python\n",
    "def perceive(self):\n",
    "    return self.current_state\n",
    "```\n",
    "\n",
    "Eso s铆, tenemos que tener cuidado para que, cuando actuemos sobre el entorno y 茅ste nos devuelva el nuevo estado, almacenarlo en este atributo. As铆, en la siguiente vuelta del bucle cuando toque percibir tendremos disponible el nuevo estado para decidir sobre 茅l. Y hablando de decidir..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decidir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando decimos 芦decidir禄 nos referimos a determinar en cada momento qu茅 acci贸n ejecutaremos dada una observaci贸n. Pero claro, ya vimos anteriormente que escoger siempre la mejor acci贸n no tiene por qu茅 ser la mejor estrategia de acci贸n. Es m谩s, a menudo las recompensas a largo plazo suelen ser mejores que las de corto plazo.\n",
    "\n",
    "Por ello, volveremos a usar la estrategia _$\\epsilon$-greedy_. En este caso delegaremos el valor del argumento `epsilon` a los m茅todos que hagan uso de esta decisi贸n\n",
    "\n",
    "```python\n",
    "def decide(self, perception, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.randrange(environment.action_space.n)\n",
    "    else:\n",
    "        perception = perception[np.newaxis, ...]\n",
    "        q_values = self.model.predict(perception, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "```\n",
    "\n",
    "Vemos que en el caso de la selecci贸n voraz, es decir, la selecci贸n de la mejor acci贸n, obtiene los _$q$-values_ de todas las opciones posibles de un modelo predictivo, escogiendo la mejor acci贸n a partir del valor m谩s alto. M谩s adelante veremos de d贸nde sale ese modelo. Por ahora vamos a terminar implementando el m茅todo destinado a actuar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actuar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este m茅todo se ejecutar谩 la acci贸n decidida. En nuestro problema en concreto delegamos totalmente en el entorno en cuesti贸n, pero podr铆a ser que el agente tuviese l贸gica adicional que hiciese que la transici贸n o la acci贸n pudiese variar.\n",
    "\n",
    "```python\n",
    "def act(self, action):\n",
    "    next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "    self.memory.remember(Transition(\n",
    "        prev_state=self.current_state,\n",
    "        next_state=next_state,\n",
    "        action=action,\n",
    "        reward=reward,\n",
    "        terminated=terminated,\n",
    "    ))\n",
    "    self.current_state = next_state\n",
    "    return self.memory[-1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aprender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu铆 es donde se actualiza el modelo. El entrenamiento pasa por obtener un _batch_ aleatorio de entre todas las transiciones que existen en la memoria y entrenar con ellas.\n",
    "\n",
    "Ahora bien, 驴qu茅 es lo que entrenamos? Veamos los fuentes en detalle\n",
    "\n",
    "```python\n",
    "def learn(self):\n",
    "    # ...\n",
    "    transactions = self.memory.batch(self.batch_size)\n",
    "\n",
    "    prev_states = np.array([t.prev_state for t in transactions])\n",
    "    next_states = np.array([t.next_state for t in transactions])\n",
    "\n",
    "    qs = self.model.predict(prev_states, verbose=0)\n",
    "    next_qs = self.model.predict(next_states, verbose=0)\n",
    "\n",
    "    for i, (txn, next_q) in enumerate(zip(transactions, next_qs)):\n",
    "        if (txn.terminated):\n",
    "            qs[i][txn.action] = txn.reward\n",
    "        else:\n",
    "            qs[i][txn.action] = txn.reward + self.gamma * np.max(next_q)    \n",
    "\n",
    "    self.model.train_on_batch(prev_states, qs)      \n",
    "```\n",
    "\n",
    "Partimos de los estados origen en las transiciones, y para ellos calculamos el $q$-value (variable `qs`) que predice la red. Una vez calculados, para cada ejemplo del _batch_ vamos a actualizar el valor del $q$-value de la acci贸n que hemos tomado en esa transici贸n en concreto con la recompensa real (o la recompensa m谩s la recompensa futura en caso de que no sea un estado terminal) para crear nuestras _labels_ con las que entrenar el modelo.\n",
    "\n",
    "En definitiva, lo que estamos haciendo es ir ajustando el modelo a predecir las salidas, y estas 煤ltimas se van ajustando a las recompensas que se han ido obteniendo a lo largo del entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto, nuestro agente tiene un modelo interno que va ajustandose al entorno de acuerdo de las vivencias del agente.\n",
    "\n",
    "Ya que una de las opciones es el de una funci贸n que recibe el n煤mero de entradas y salidas del modelo requerido, vamos a implementar un peque帽o modelo que trate de aprender el problema en cuesti贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_inputs, num_outputs) -> tf.keras.models.Model:\n",
    "    \"\"\"Crea un nuevo modelo con nuestro agente.\"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(24,\n",
    "                              activation='relu',\n",
    "                              kernel_initializer='he_normal',\n",
    "                              input_shape=num_inputs),\n",
    "        tf.keras.layers.Dense(24,\n",
    "                              activation='relu',\n",
    "                              kernel_initializer='he_normal'),\n",
    "        tf.keras.layers.Dense(num_outputs, activation='linear'),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_model(num_inputs, num_outputs) -> tf.keras.models.Model:\n",
    "    \"\"\"Crea un nuevo modelo con nuestro agente.\"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal', input_shape=(num_inputs,)),\n",
    "        tf.keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
    "        tf.keras.layers.Dense(num_outputs, activation='linear'),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbbw6blhDcsJ"
   },
   "source": [
    "## Entrenamiento del modelo\n",
    "\n",
    "Ahora s铆, ya tenemos todas las piezas disponibles para que nuestro m贸dulo lunar aprenda a desenvolverse en el entorno. Comenzaremos creando el entorno y el agente que trabajar谩 sobre 茅l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gymnasium.make('CartPole-v1')\n",
    "agent = Agent(env=environment, model=build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, entrenaremos el agente. Para ello iteraremos un n煤mero determinado de tareas durante un n煤mero determinado de pasos (para no pasarnos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yuzI0m5u5vVf",
    "outputId": "c741ec8a-fc05-4369-96b4-5bd5347d2e4f"
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_EPISODES = 1000\n",
    "MAX_STEPS = 450\n",
    "\n",
    "RUNNING_AVG_WINDOW_SIZE = 10\n",
    "\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "DEC_EPSILON = 0.995\n",
    "\n",
    "rewards = []\n",
    "rewards_avg = []\n",
    "epsilon = MAX_EPSILON\n",
    "for episode in range(NUMBER_OF_EPISODES):\n",
    "    # Reseteamos el entorno y el agente para comenzar un nuevo episodio\n",
    "    reward = agent.episode(epsilon, MAX_STEPS)\n",
    "\n",
    "    # Salvamos el modelo\n",
    "    agent.model.save(f'tmp/cart-pole.h5')\n",
    "\n",
    "    # Actualizamos el hist贸rico de valores\n",
    "    rewards.append(reward)\n",
    "    rewards_avg.append(np.mean(rewards[-RUNNING_AVG_WINDOW_SIZE:]))\n",
    "    \n",
    "    # Imprimimos un mensaje\n",
    "    print(f'Episode: {episode}, reward: {reward:5.4} (best: {max(rewards):5.4}, avg. window: {rewards_avg[-1]:5.4})', end='\\r')\n",
    "\n",
    "    # Disminuimos  para reducir la aletoriedad de la selecci贸n de acci贸n\n",
    "    epsilon *= DEC_EPSILON\n",
    "    epsilon = max(MIN_EPSILON, epsilon)\n",
    "    \n",
    "    # Clear de la sesi贸n porque con fit y predict se quedan nodos del grafo\n",
    "    # referenciados en memoria que nunca se limpian y, por tanto, me consumen\n",
    "    # toda la memoria despu茅s de bastantes vueltas del bucle.\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a imprimir la evoluci贸n de las recompensas para ver c贸mo han evolucionado a lo largo del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title(f'Episode {episode}, max. reward = {max(rewards):5.4f} on episode {np.argmax(rewards)})')\n",
    "ax.plot(rewards, linewidth=0.5, label='Instant reward')\n",
    "ax.plot(rewards_avg, linewidth=1.5, label=f'10 steps average')\n",
    "ax.legend()\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluaci贸n del agente en una instancia del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos c贸mo se comporta el agente en una neuva instancia del problema. En este caso visualizaremos el comportamiento para comprobar si lo resuelve de forma satisfactoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "state, _ = env.reset()\n",
    "terminated = truncated = False\n",
    "while not (terminated or truncated):\n",
    "    # Seleccionamos una acci贸n dado el actual estado ...\n",
    "    action = np.argmax(agent.model.predict(state[np.newaxis, ...], verbose=0)[0])\n",
    "    # ... y la ejecutamos, obteniendo el nuevo estado\n",
    "    state, _, terminated, truncated, _ = env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos demostrado que implementar un agente para resolver el problema del _Cart Pole_ (el p茅ndulo invertido) es un poco tedioso, pero nada demasiado complicado. Nuestro agente aprendi贸 a equilibrar el p茅ndulo utilizando la informaci贸n proporcionada por el entorno (esto es, posici贸n y velocidad del carro, inclinaci贸n y velocidad angular del p茅ndulo).\n",
    "\n",
    "Adem谩s, tambi茅n hemos explorado los principales hiperpar谩metros que afectan a este tipo de algoritmos, como la tasa de aprendizaje, el factor de descuento y el tama帽o del _batch_, para optimizar el rendimiento del agente.\n",
    "\n",
    "En general, el Deep Q Learning es una t茅cnica poderosa y vers谩til para entrenar agentes de aprendizaje por refuerzo en una amplia gama de problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Example DQN Cartpole.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
