{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TT3LPGc5PuK"
   },
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Resolviendo el problema Cart Pole con Deep $Q$-learning<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Última actualización: 2023-04-19</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXBzOdaLAEUn"
   },
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación más sencilla del $Q$-Learning utiliza una tabla para guardar todos los valores, pero esto se convierte rápidamente en inviable para entornos con muchos estados o acciones posibles. Un entorno con un espacio de observaciones continuo (prácticamente cualquiera) es uno de esos ejemplos, ya que discrretizar el espacio de acciones dispara el espacio requerido para la tabla en memoria.\n",
    "\n",
    "Utilizando un modelo de red neuronal (profunda) para representar la función $Q(s, a)$, el agente puede generalizar mejor combinaciones no experimentadas de $(s, a)$. La entrada a este modelo sería una observación, mientras que la salida sería un valor $Q$ para cada acción permitida en ese estado. Esta implementación se denomina Deep $Q$-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-YSpziT0K9I"
   },
   "source": [
    "Vamos a tratar de resolver el problema del péndulo invertido que se implementan en el entorno [Cart Pole de Gymnasium](https://www.gymlibrary.dev/environments/classic_control/cart_pole/). Aplicaremos la técnica de _Deep $Q$-learning_, la cual es la indicada en el caso de procesos en los que el espacio de estados en el que trabajar es continuo.\n",
    "\n",
    "![](https://blazaid.github.io/Aprendizaje-profundo/Notebooks//cart-pole.gif)\n",
    "\n",
    "El espacio de estados se representa con un vector de 4 dimensiones donde las posiciones representan lo siguiente:\n",
    "0. Posición del carrito (de -4.8 a 4.8)\n",
    "1. Velocidad del carrito (de $-\\infty$ a $\\infty$)\n",
    "2. Ángulo del péndulo (de -0.418 a 0.418)\n",
    "3. Velocidad angular del péndulo (de $-\\infty$ a $\\infty$)\n",
    "\n",
    "El espacio de acciones es un vector de dos dimensiones donde cada posición representa:\n",
    "\n",
    "0. Empujar al carrito a la izquierda\n",
    "1. Empujar el carrito a la derecha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación importaremos las bibliotecas que se usarán a lo largo del _notebook_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjLS1WetFhCE"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import os\n",
    "import random\n",
    "from typing import Any, Callable, List, NamedTuple, Sequence, SupportsFloat, Union\n",
    "\n",
    "# Desactivamos los warnings de tensorflow, que son un poco cargantes\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asimismo, configuramos algunos parámetros para adecuar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (15, 8),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementos auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la implementación será un poco más larga que el _notebook_ anterior, haremos uso de una serie de elementos auxiliares que nos ayudarán ha hacer el código más manejable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transiciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos definiendo la clase `Transition`, que representará la transición que ocurre de un estado $s_t$ a un estado $s_{t+1}$.\n",
    "\n",
    "Dicha transición contendrá la información de qué acción provocó la transición, cuál fue la recompensa de haberla llevado a cabo y si se ha terminado, ya sea por llegar a un estado terminal (p.ej. llegar al destino o morir) o por cualquier otra razón (p.ej llegar a un determinado límite de tiempo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(NamedTuple):\n",
    "    \"\"\"Representa la transición de un estado al siguiente\"\"\"\n",
    "    prev_state: Any       # Estado origen de la transición\n",
    "    next_state: Any       # Estado destino de la transición\n",
    "    action: Any           # Acción que provocó esta transición\n",
    "    reward: SupportsFloat # Recompensa obtenida\n",
    "    terminated: bool      # Si se ha llegado a un estado terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las propiedades con las que cuentan los agentes al ser entrenados con $Q$-Learning es la memoria de experiencias, representadas como transiciones entre estados. Ésta se usará durante la fase de entrenamiento.\n",
    "\n",
    "La implementaremos como un buffer de tamaño opcionalmente limitado. Generalmente se acota el tamaño de la memoria, reemplazando las memorias más antigüas con las más recientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"Representa la memoria de un agente.\n",
    "\n",
    "    Concretamente, almacenará las últimas n transiciones realizadas en\n",
    "    el entorno. El tamaño de la memoria se establecerá en el momento de\n",
    "    crear la memoria del mismo.\n",
    "\n",
    "    La memoria guarda las transiciones de manera ordenada, y se podrá\n",
    "    acceder a ellos por índice, de manera que el recuerdo más lejano\n",
    "    estará en la posición 0 y el más reciente en la posición -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int):\n",
    "        \"\"\"Inicializa el objeto.\n",
    "\n",
    "        :param size: El tamaño máximo de la memoria del agente.\"\"\"\n",
    "        self.max_size = int(size)\n",
    "        self.transitions: deque = deque(maxlen=self.max_size)\n",
    "\n",
    "    def remember(self, transition: Transition):\n",
    "        \"\"\"Añade un nuevo recuerdo a la memoria del agente.\n",
    "\n",
    "        :param transition: La transición a recordar.\"\"\"\n",
    "        self.transitions.append(transition)\n",
    "\n",
    "    def batch(self, n: int) -> List[Transition]:\n",
    "        \"\"\"Devuelve n recuerdos aleatorios de la memoria.\n",
    "\n",
    "        :param n: El número de recuerdos aleatorios a devolver. Si es\n",
    "            superior al número de recuerdos totales devolverá todos los\n",
    "            recuerdos almacenados.\n",
    "        :returns: La lista de transiciones.\n",
    "        \"\"\"\n",
    "        n = min(len(self.transitions), n)\n",
    "        return random.sample(self.transitions, n)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"El número de recuerdos que contiene esta memoria.\n",
    "\n",
    "        :returns: Un entero mayor o igual a 0.\n",
    "        \"\"\"\n",
    "        return len(self.transitions)\n",
    "\n",
    "    def __getitem__(\n",
    "            self,\n",
    "            key: Union[int, slice]\n",
    "    ) -> Union[Transition, Sequence[Transition]]:\n",
    "        \"\"\"Devuelve el/los elemento/s especificados.\n",
    "\n",
    "        :param key: El argumento que indica los elementos. Puede ser un\n",
    "            entero normal o un slice.\n",
    "        :returns: El/los elemento/s especificados por el índice.\n",
    "        \"\"\"\n",
    "        return self.transitions.__getitem__(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro agente se parecerá a los agentes que hemos implementado. Primero, porque comparten el hecho de que se tratan de agentes interactuando con un entorno (percibir $\\rightarrow$ actuar), y segundo, porque _Deep $Q$-learning_ es una extensión de _$Q$-learning_, por lo que algo se tiene que parecer.\n",
    "\n",
    "Vamos con la implementación y posteriormente explicaremos los detalles más importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZ6P4Gj0FtnU"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(\n",
    "        self, *,\n",
    "        env: gymnasium.Env,\n",
    "        model: Union[Callable[[int, int], tf.keras.Model], tf.keras.Model, str],\n",
    "        batch_size=32,\n",
    "        memory_size: int = 1e5,\n",
    "        gamma=0.99,\n",
    "    ):\n",
    "        \"\"\"Inicializa el objeto.\n",
    "        :param model: El modelo del objeto. Puede ser una función que devuelva\n",
    "            un nuevo modelo (compilado), un objeto de `tf.keras.Model` ya\n",
    "            existente (en cuyo caso se clonará) o una cadena con un path\n",
    "            válido, en cuyo caso se cargará de disco.\n",
    "        \"\"\"\n",
    "        # El entorno en el que vamos a trabajar\n",
    "        self.env = env\n",
    "        self.num_inputs = env.observation_space.shape[0]\n",
    "        self.num_outputs = env.action_space.n\n",
    "\n",
    "        # El modelo de aprendizaje de nuestro agente\n",
    "        if callable(model):\n",
    "            self.model = model(\n",
    "                env.observation_space.shape[0],\n",
    "                env.action_space.n,\n",
    "            )\n",
    "        elif isinstance(model, tf.keras.models.Model):\n",
    "            self.model = model\n",
    "        elif isinstance(model, str):\n",
    "            self.model = tf.keras.models.load_model(model)\n",
    "        else:\n",
    "            raise ValueError('Valid models are a function, a model or a path')\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.memory = Memory(size=1e5)\n",
    "\n",
    "        # El estado del entorno en el que se encuentra el agente\n",
    "        self.current_state = None\n",
    "        # El número de paso en la tarea en la que se encuentra el agente\n",
    "        self.current_step = 0\n",
    "\n",
    "    def episode(self, epsilon=0, max_iterations=None):\n",
    "        max_iterations = max_iterations or np.inf\n",
    "\n",
    "        self.current_state, _ = self.env.reset()\n",
    "        self.current_step = 0\n",
    "\n",
    "        reward = 0\n",
    "        running = True\n",
    "        while running and self.current_step < max_iterations:\n",
    "            self.current_step += 1\n",
    "\n",
    "            perception = self.perceive()\n",
    "            action = self.decide(perception, epsilon)\n",
    "            transition = self.act(action)\n",
    "            self.learn()\n",
    "\n",
    "            reward += transition.reward\n",
    "            running = not transition.terminated\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def perceive(self):\n",
    "        return self.current_state\n",
    "        \n",
    "    def decide(self, perception, epsilon=0):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return random.randrange(environment.action_space.n)\n",
    "        else:\n",
    "            perception = perception[np.newaxis, ...]\n",
    "            q_values = self.model.predict(perception, verbose=0)\n",
    "            return np.argmax(q_values[0])\n",
    "    \n",
    "    def act(self, action):\n",
    "        # Ejecutamos la acción sobre el entorno\n",
    "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Guardamos en la memoria del agente la transición realizada junto con\n",
    "        # su información\n",
    "        self.memory.remember(Transition(\n",
    "            prev_state=self.current_state,\n",
    "            next_state=next_state,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            terminated=terminated,\n",
    "        ))\n",
    "        \n",
    "        # Indicamos el nuevo estado en el que se encuentra el agente ahora\n",
    "        self.current_state = next_state\n",
    "        \n",
    "        return self.memory[-1]\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            transactions = self.memory.batch(self.batch_size)\n",
    "            \n",
    "            prev_states = np.array([t.prev_state for t in transactions])\n",
    "            next_states = np.array([t.next_state for t in transactions])\n",
    "\n",
    "            qs = self.model.predict(prev_states, verbose=0)\n",
    "            next_qs = self.model.predict(next_states, verbose=0)\n",
    "\n",
    "            for i, (txn, next_q) in enumerate(zip(transactions, next_qs)):\n",
    "                if (txn.terminated):\n",
    "                    qs[i][txn.action] = txn.reward\n",
    "                else:\n",
    "                    qs[i][txn.action] = txn.reward + self.gamma * np.max(next_q)    \n",
    "\n",
    "            self.model.train_on_batch(prev_states, qs)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a recorrer ciertos detalles de la implementación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En entrenamiento supervisado y no supervisado, los _epochs_ son una vuelta entera sobre todo el conjunto de datos para el entrenamiento del modelo. En aprendizaje por refuerzo su homólogo son los **episodios** o tareas, los cuales son una ejecución del agente sobre el entorno hasta que éste finaliza su tarea (buen o mal). Esta ejecución se realiza en el método `episode`\n",
    "\n",
    "El agente, para aprender, ejecutará tareas una detrás de otra. Cada una de ellas constará de un ciclo constante de percibir (método `perceive`) el entorno, decidir la acción a ejecutar (método `decide`), ejecutar dicha acción sobre el entorno (método `act`), y aprender sobre lo vivido (método `learn`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percibir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro problema vamos a usar como percepción exclusivamente el estado devuelto por el entorno, aunque en otros casos podría usarse otra información adicional (por ejemplo el número de iteraciones que llevamos dentro del episodio para que aprenda a tener prisa si se está acabando el tiempo, por ejemplo).\n",
    "\n",
    "\n",
    "```python\n",
    "def perceive(self):\n",
    "    return self.current_state\n",
    "```\n",
    "\n",
    "Eso sí, tenemos que tener cuidado para que, cuando actuemos sobre el entorno y éste nos devuelva el nuevo estado, almacenarlo en este atributo. Así, en la siguiente vuelta del bucle cuando toque percibir tendremos disponible el nuevo estado para decidir sobre él. Y hablando de decidir..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decidir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando decimos «decidir» nos referimos a determinar en cada momento qué acción ejecutaremos dada una observación. Pero claro, ya vimos anteriormente que escoger siempre la mejor acción no tiene por qué ser la mejor estrategia de acción. Es más, a menudo las recompensas a largo plazo suelen ser mejores que las de corto plazo.\n",
    "\n",
    "Por ello, volveremos a usar la estrategia _$\\epsilon$-greedy_. En este caso delegaremos el valor del argumento `epsilon` a los métodos que hagan uso de esta decisión\n",
    "\n",
    "```python\n",
    "def decide(self, perception, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.randrange(environment.action_space.n)\n",
    "    else:\n",
    "        perception = perception[np.newaxis, ...]\n",
    "        q_values = self.model.predict(perception, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "```\n",
    "\n",
    "Vemos que en el caso de la selección voraz, es decir, la selección de la mejor acción, obtiene los _$q$-values_ de todas las opciones posibles de un modelo predictivo, escogiendo la mejor acción a partir del valor más alto. Más adelante veremos de dónde sale ese modelo. Por ahora vamos a terminar implementando el método destinado a actuar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actuar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este método se ejecutará la acción decidida. En nuestro problema en concreto delegamos totalmente en el entorno en cuestión, pero podría ser que el agente tuviese lógica adicional que hiciese que la transición o la acción pudiese variar.\n",
    "\n",
    "```python\n",
    "def act(self, action):\n",
    "    next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "    self.memory.remember(Transition(\n",
    "        prev_state=self.current_state,\n",
    "        next_state=next_state,\n",
    "        action=action,\n",
    "        reward=reward,\n",
    "        terminated=terminated,\n",
    "    ))\n",
    "    self.current_state = next_state\n",
    "    return self.memory[-1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aprender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí es donde se actualiza el modelo. El entrenamiento pasa por obtener un _batch_ aleatorio de entre todas las transiciones que existen en la memoria y entrenar con ellas.\n",
    "\n",
    "Ahora bien, ¿qué es lo que entrenamos? Veamos los fuentes en detalle\n",
    "\n",
    "```python\n",
    "def learn(self):\n",
    "    # ...\n",
    "    transactions = self.memory.batch(self.batch_size)\n",
    "\n",
    "    prev_states = np.array([t.prev_state for t in transactions])\n",
    "    next_states = np.array([t.next_state for t in transactions])\n",
    "\n",
    "    qs = self.model.predict(prev_states, verbose=0)\n",
    "    next_qs = self.model.predict(next_states, verbose=0)\n",
    "\n",
    "    for i, (txn, next_q) in enumerate(zip(transactions, next_qs)):\n",
    "        if (txn.terminated):\n",
    "            qs[i][txn.action] = txn.reward\n",
    "        else:\n",
    "            qs[i][txn.action] = txn.reward + self.gamma * np.max(next_q)    \n",
    "\n",
    "    self.model.train_on_batch(prev_states, qs)      \n",
    "```\n",
    "\n",
    "Partimos de los estados origen en las transiciones, y para ellos calculamos el $q$-value (variable `qs`) que predice la red. Una vez calculados, para cada ejemplo del _batch_ vamos a actualizar el valor del $q$-value de la acción que hemos tomado en esa transición en concreto con la recompensa real (o la recompensa más la recompensa futura en caso de que no sea un estado terminal) para crear nuestras _labels_ con las que entrenar el modelo.\n",
    "\n",
    "En definitiva, lo que estamos haciendo es ir ajustando el modelo a predecir las salidas, y estas últimas se van ajustando a las recompensas que se han ido obteniendo a lo largo del entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto, nuestro agente tiene un modelo interno que va ajustandose al entorno de acuerdo de las vivencias del agente.\n",
    "\n",
    "Ya que una de las opciones es el de una función que recibe el número de entradas y salidas del modelo requerido, vamos a implementar un pequeño modelo que trate de aprender el problema en cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_inputs, num_outputs) -> tf.keras.models.Model:\n",
    "    \"\"\"Crea un nuevo modelo con nuestro agente.\"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(24,\n",
    "                              activation='relu',\n",
    "                              kernel_initializer='he_normal',\n",
    "                              input_shape=num_inputs),\n",
    "        tf.keras.layers.Dense(24,\n",
    "                              activation='relu',\n",
    "                              kernel_initializer='he_normal'),\n",
    "        tf.keras.layers.Dense(num_outputs, activation='linear'),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_model(num_inputs, num_outputs) -> tf.keras.models.Model:\n",
    "    \"\"\"Crea un nuevo modelo con nuestro agente.\"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal', input_shape=(num_inputs,)),\n",
    "        tf.keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
    "        tf.keras.layers.Dense(num_outputs, activation='linear'),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbbw6blhDcsJ"
   },
   "source": [
    "## Entrenamiento del modelo\n",
    "\n",
    "Ahora sí, ya tenemos todas las piezas disponibles para que nuestro módulo lunar aprenda a desenvolverse en el entorno. Comenzaremos creando el entorno y el agente que trabajará sobre él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gymnasium.make('CartPole-v1')\n",
    "agent = Agent(env=environment, model=build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, entrenaremos el agente. Para ello iteraremos un número determinado de tareas durante un número determinado de pasos (para no pasarnos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yuzI0m5u5vVf",
    "outputId": "c741ec8a-fc05-4369-96b4-5bd5347d2e4f"
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_EPISODES = 1000\n",
    "MAX_STEPS = 450\n",
    "\n",
    "RUNNING_AVG_WINDOW_SIZE = 10\n",
    "\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "DEC_EPSILON = 0.995\n",
    "\n",
    "rewards = []\n",
    "rewards_avg = []\n",
    "epsilon = MAX_EPSILON\n",
    "for episode in range(NUMBER_OF_EPISODES):\n",
    "    # Reseteamos el entorno y el agente para comenzar un nuevo episodio\n",
    "    reward = agent.episode(epsilon, MAX_STEPS)\n",
    "\n",
    "    # Salvamos el modelo\n",
    "    agent.model.save(f'tmp/cart-pole.h5')\n",
    "\n",
    "    # Actualizamos el histórico de valores\n",
    "    rewards.append(reward)\n",
    "    rewards_avg.append(np.mean(rewards[-RUNNING_AVG_WINDOW_SIZE:]))\n",
    "    \n",
    "    # Imprimimos un mensaje\n",
    "    print(f'Episode: {episode}, reward: {reward:5.4} (best: {max(rewards):5.4}, avg. window: {rewards_avg[-1]:5.4})', end='\\r')\n",
    "\n",
    "    # Disminuimos 𝜀 para reducir la aletoriedad de la selección de acción\n",
    "    epsilon *= DEC_EPSILON\n",
    "    epsilon = max(MIN_EPSILON, epsilon)\n",
    "    \n",
    "    # Clear de la sesión porque con fit y predict se quedan nodos del grafo\n",
    "    # referenciados en memoria que nunca se limpian y, por tanto, me consumen\n",
    "    # toda la memoria después de bastantes vueltas del bucle.\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a imprimir la evolución de las recompensas para ver cómo han evolucionado a lo largo del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title(f'Episode {episode}, max. reward = {max(rewards):5.4f} on episode {np.argmax(rewards)})')\n",
    "ax.plot(rewards, linewidth=0.5, label='Instant reward')\n",
    "ax.plot(rewards_avg, linewidth=1.5, label=f'10 steps average')\n",
    "ax.legend()\n",
    "plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del agente en una instancia del problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo se comporta el agente en una neuva instancia del problema. En este caso visualizaremos el comportamiento para comprobar si lo resuelve de forma satisfactoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "state, _ = env.reset()\n",
    "terminated = truncated = False\n",
    "while not (terminated or truncated):\n",
    "    # Seleccionamos una acción dado el actual estado ...\n",
    "    action = np.argmax(agent.model.predict(state[np.newaxis, ...], verbose=0)[0])\n",
    "    # ... y la ejecutamos, obteniendo el nuevo estado\n",
    "    state, _, terminated, truncated, _ = env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos demostrado que implementar un agente para resolver el problema del _Cart Pole_ (el péndulo invertido) es un poco tedioso, pero nada demasiado complicado. Nuestro agente aprendió a equilibrar el péndulo utilizando la información proporcionada por el entorno (esto es, posición y velocidad del carro, inclinación y velocidad angular del péndulo).\n",
    "\n",
    "Además, también hemos explorado los principales hiperparámetros que afectan a este tipo de algoritmos, como la tasa de aprendizaje, el factor de descuento y el tamaño del _batch_, para optimizar el rendimiento del agente.\n",
    "\n",
    "En general, el Deep Q Learning es una técnica poderosa y versátil para entrenar agentes de aprendizaje por refuerzo en una amplia gama de problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Example DQN Cartpole.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
